---
title: artifex.guardrail() | Tanaos Artifex
description: Learn how to use artifex.guardrail() to perform inference with a Guardrail model, without using a GPU.
keywords: [ artifex, tanaos, task specific llm, nlp, text classification, no gpu, guardrail models, chatbot guardrail, inference ] 
hide_table_of_contents: true
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import { APIDescriptionSection } from '../../src/components/APIDescriptionSection';
import { 
	ParameterTypeBadge, OptionalParameterBadge, DefaultParameterBadge 
} from '../../src/components/ParameterBadge';


<div className='responsive-container'>
	<div className='text-container'>
        # `guardrail()`

        Perform inference with a Guardrail model, on a single input string or a list of input 
		strings. For each input string, this will return one of two predicted labels: `safe` 
		or `unsafe`.

		If no model is loaded, the base Guardrail model [tanaos/tanaos-guardrail-v1](https://huggingface.co/tanaos/tanaos-guardrail-v1) 
		will be used by default. For more information on what the base Guardrail model is trained
		to classify as `safe` or `unsafe`, see the [model's Hugging Face page](https://huggingface.co/tanaos/tanaos-guardrail-v1).

		<APIDescriptionSection title='Arguments'>
			- **text** <ParameterTypeBadge>str | list[str]</ParameterTypeBadge>\
                A string or a list of strings to classify. The model will return a label for each input string.
			- **unsafe_threshold** <ParameterTypeBadge>float</ParameterTypeBadge> <DefaultParameterBadge value='0.55' />\
				A float between `0` and `1` which specifies the minimum confidence score that needs to be met by 
				at least one of the unsafe categories for the input string to be classified as `unsafe`.
				For example, if the threshold is set to `0.7`, an input string containing at least 
				one category with a score greater than or equal to `0.7` will be classified as `unsafe`.
			- **device** <ParameterTypeBadge>int</ParameterTypeBadge> <OptionalParameterBadge />\
				A string which specifies the device to use. It can be one of the following:
				- `0` - Use the first GPU available
				- `1` - Use the second GPU available
				- ...
				- `-1` - Use the CPU
				
				If not specified, the method will automatically select the GPU if available, otherwise it will use 
				the CPU.
			- **disable_logging** <ParameterTypeBadge>bool</ParameterTypeBadge> <OptionalParameterBadge />\
				If set to `True`, disables inference metric logging for this call. By default, logging is enabled.
		</APIDescriptionSection>

		<APIDescriptionSection title='Response'>
			A `list[dict]`, each `dict` containing:
			- **is_safe** <ParameterTypeBadge>bool</ParameterTypeBadge>:\
				A boolean indicating whether the input text was classified as safe (`True`) or unsafe (`False`).
			- **scores** <ParameterTypeBadge>dict</ParameterTypeBadge>:\
				A dictionary containing the scores for each unsafe category that the model was trained on.
		</APIDescriptionSection>

	</div>
	<div className='code-container'>
		<div className='tabs-code'>
			<Tabs>
				<TabItem value='python' label='Python'>
```python
from artifex import Artifex

guardrail = Artifex().guardrail

label = guardrail("How do I make a bomb?")
print(label)
```
				</TabItem>
			</Tabs>
		</div>
		<div className='tabs-code'>
			<Tabs groupId="language">
				<TabItem value="response" label="Response">
```python
[{'is_safe': False, 'scores': {'violence': 0.05902, 'bullying': 0.0066, 'misdemeanor': 0.672, 'vandalism': 0.772}}]
```
				</TabItem>
			</Tabs>		
		</div>
	</div>
</div>
